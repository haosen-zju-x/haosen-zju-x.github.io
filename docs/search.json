[{"categories":["Statistique"],"content":" Exercices Test de comparaison de deux variances \u0026 Test de comparaison de deux moyennes Considérons le modèle statistique \\[ \\left(\\mathbb{R}^{n+p},\\mathcal{B}(\\mathbb{R}^{n+p}),\\left\\{\\mathbb{P}_{\\theta}=p_{\\theta}\\cdot\\mathrm{d}\\text{Leb}^{\\otimes (n+p)}=(\\mathcal{N}(\\mu_0,\\sigma_0^2))^{\\otimes n}\\otimes (\\mathcal{N}(\\mu_1,\\sigma_1^2))^{\\otimes p} \\,\\Big|\\, \\theta = (\\mu_0,\\mu_1,\\sigma_0^2,\\sigma_1^2)\\in \\mathbb{R}^2\\times (\\mathbb{R}_+^*)^2\\right\\}\\right). \\]\n1. Déterminer l’estimateur du maximum de vraisemblance du paramètre \\(\\theta=(\\mu_0,\\mu_1,\\sigma_0^2,\\sigma_1^2)\\). On remarque que \\(L(\\theta;X_1,\\cdots,X_n,Y_1,\\cdots,Y_p)=L(\\theta_0;X_1,\\cdots,X_n)\\cdot L(\\theta_1;Y_1,\\cdots,Y_p)\\), où \\(\\theta_0=(\\mu_0,\\sigma_0^2)\\) et \\(\\theta_1=(\\mu_1,\\sigma_1^2)\\). Comme l’estimateur du maximum de vraisemblance du paramètre \\(\\theta_0\\) est \\(\\left(\\bar{X}_n,V_n\\right)\\), où \\[ \\bar{X}_n:=\\frac{1}{n}\\sum_{i=1}^{n}X_i,\\quad V_n:=\\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\bar{X}_n)^2, \\] et que l’estimateur du maximum de vraisemblance du paramètre \\(\\theta_1\\) est \\((\\bar{Y}_p,W_p)\\), où \\[ \\bar{Y}_p:=\\frac{1}{p}\\sum_{j=1}^{n}Y_j,\\quad W_p:=\\frac{1}{p}\\sum_{j=1}^{p}(Y_j-\\bar{Y}_p)^2, \\] on en conclut que l’estimateur du maximum de vraisemblance du paramètre \\(\\theta\\) est \\[ \\hat{\\theta}_{n,p}^{\\text{MV}}=(\\bar{X}_n,\\bar{Y}_p,V_n,W_p). \\]\n2. Déterminer un test de niveau \\(\\alpha\\) de l’hypothèse \\[ H_0 : \\sigma_0^2=\\sigma_1^2,\\quad\\text{contre}\\quad H_1 : \\sigma_0^2\u003e\\sigma_1^2. \\] Rappelons que sous \\(\\mathbb{P}_{\\theta}\\), \\(nV_n/\\sigma_0^2=\\frac{1}{\\sigma_0^2}\\sum_{i=1}^{n}(X_i-\\bar{X}_n)^2\\sim \\chi^2(n-1)\\) et \\(pW_p/\\sigma_1^2=\\frac{1}{\\sigma_1^2}\\sum_{j=1}^{p}(Y_j-\\bar{Y}_p)^2\\sim \\chi^2(p-1)\\). Alors, pour tout \\(\\theta\\in \\Theta\\triangleq\\mathbb{R}^2\\times (\\mathbb{R}_+^*)^2\\), on a sous \\(\\mathbb{P}_{\\theta}\\) \\[ \\frac{nV_n/\\sigma_0^2}{pW_p/\\sigma_1^2}\\sim F(n-1,p-1), \\] ce qui ne dépend pas du paramètre \\(\\theta\\). Par conséquent, sous \\(H_0\\) (qui suppose que \\(\\sigma_0^2=\\sigma_1^2\\)) on a \\[ \\mathbb{P}_{\\theta}\\left(\\frac{nV_n}{pW_p}\\le q_{1-\\alpha}^{F(n-1,p-1)}\\right)=1-\\alpha. \\] Ainsi, \\(\\phi(Z):=\\mathbb{1}\\left\\{\\dfrac{nV_n}{pW_p}\\ge q_{1-\\alpha}^{F(n-1,p-1)}\\right\\}\\) est un test de taille (et donc de niveau) \\(\\alpha\\).\n3. Calculer la \\(p\\)-valeur du test. Dans notre cas, la statistique de test est \\(T(Z):=\\frac{nV_n}{pW_p}\\) et la valeur critique du test est \\(c_{\\alpha}:=q_{1-\\alpha}^{F(n-1,p-1)}\\), qui est décroissante par rapport à \\(\\alpha\\in [0,1]\\). Ainsi, la \\(p\\)-valeur du test est \\[ \\begin{align*} \\hat{\\alpha}(Z) \u0026=\\inf\\{\\alpha\\in ]0,1[ \\,:\\, T(Z)\\ge c_{\\alpha}\\}\\\\ \u0026=1-\\Phi\\left(\\frac{nV_n}{pW_p}\\right), \\end{align*} \\] où \\(\\Phi\\) désigne la fonction de répartition de la loi de Fisher de \\((n-1,p-1)\\) degrés de liberté.\nNous considérons dans la suite le modèle statistique \\[ \\left(\\mathbb{R}^{n+p},\\mathcal{B}(\\mathbb{R}^{n+p}),\\left\\{\\mathbb{P}_{\\theta}=p_{\\theta}\\cdot\\mathrm{d}\\text{Leb}^{\\otimes (n+p)}=(\\mathcal{N}(\\mu_0,\\sigma^2))^{\\otimes n}\\otimes (\\mathcal{N}(\\mu_1,\\sigma^2))^{\\otimes p} \\,\\Big|\\, \\theta = (\\mu_0,\\mu_1,\\sigma^2)\\in \\mathbb{R}^2\\times \\mathbb{R}_+^*\\right\\}\\right). \\]\n4. Construire un intervalle de confiance de \\(\\mu_0-\\mu_1\\) de niveau de confiance \\(1-\\alpha\\). Sous \\(\\mathbb{P}_{\\theta}\\), les deux variables aléatoires réelles \\(\\bar{X}_n\\sim \\mathcal{N}(\\mu_0,\\sigma^2/n)\\) et \\(\\bar{Y}_p\\sim \\mathcal{N}(\\mu_1,\\sigma^2/p)\\) sont indépendentes, et donc \\[ \\frac{(\\bar{X}_n-\\bar{Y}_p)-(\\mu_0-\\mu_1)}{\\sqrt{\\sigma^2/n+\\sigma^2/p}}\\sim \\mathcal{N}(0,1). \\] D’autre part, on sait que sous \\(\\mathbb{P}_{\\theta}\\), \\[ \\frac{1}{\\sigma^2}(nV_n+pW_p)\\sim \\chi^2(n+p-2). \\] On rappelle aussi que \\(\\bar{X}_n,V_n\\) sont indépendantes sous \\(\\mathbb{P}_{\\theta}\\), et que \\(\\bar{Y}_p,W_p\\) le sont aussi. En plus, comme \\(X_1,\\cdots,X_n\\) et \\(Y_1,\\cdots,Y_p\\) sont indépendantes entre elles sous \\(\\mathbb{P}_{\\theta}\\), on voit que \\(\\bar{X}_n,W_p\\) sont indépendantes sous \\(\\mathbb{P}_{\\theta}\\), et que \\(\\bar{Y}_p,V_n\\) le sont aussi. Donc, les deux variables aléatoires réelles nouvellement construites, \\(\\frac{(\\bar{X}_n-\\bar{Y}_p)-(\\mu_0-\\mu_1)}{\\sqrt{\\sigma^2/n+\\sigma^2/p}}\\) et \\(\\frac{1}{\\sigma^2}(nV_n+pW_p)\\), sont indépendantes sous \\(\\mathbb{P}_{\\theta}\\). Cela permet de conclure que sous \\(\\mathbb{P}_{\\theta}\\), \\[ \\frac{[(\\bar{X}_n-\\bar{Y}_p)-(\\mu_0-\\mu_1)]^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\sim F(1,n+p-2). \\] Alors, pour tout \\(\\theta\\in \\Theta\\), \\[ \\mathbb{P}_{\\theta}\\left(\\frac{[(\\bar{X}_n-\\bar{Y}_p)-(\\mu_0-\\mu_1)]^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\le q_{1-\\alpha}^{F(1,n+p-2)}\\right)=1-\\alpha. \\] On en tire donc une intervalle de confiance de niveau de confiance \\(1-\\alpha\\) pour \\(\\mu_0-\\mu_1\\) : \\[ \\textstyle \\left[(\\bar{X}_n-\\bar{Y}_p)\\pm \\sqrt{nV_n+pW_p}\\sqrt{(\\frac{1}{n}+\\frac{1}{p})q_{1-\\alpha}^{F(1,n+p-2)}}\\right] \\]\n5. En déduire un test de niveau \\(\\alpha\\) de l’hypothèse \\[ H_0 : \\mu_0=\\mu_1,\\quad\\text{contre}\\quad H_1 : \\mu_0\\neq \\mu_1. \\] D’après la question précédente, le test défini par \\[ \\phi(Z):=\\mathbb{1}\\left\\{\\frac{(\\bar{X}_n-\\bar{Y}_p)^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\ge q_{1-\\alpha}^{F(1,n+p-2)}\\right\\} \\] est un test de taille (et donc de niveau) \\(\\alpha\\).\n6. Calculer la \\(p\\)-valeur du test. Dans notre cas, la statistique de test est \\(T(Z):=\\frac{(\\bar{X}_n-\\bar{Y}_p)^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\), et la valeur critique du test est \\(c_{\\alpha}:=q_{1-\\alpha}^{F(1,n+p-2)}\\), qui est décroissante par rapport à \\(\\alpha\\in [0,1]\\). Ainsi, la \\(p\\)-valeur du test est \\[ \\begin{align*} \\hat{\\alpha}(Z) \u0026=\\inf\\{\\alpha\\in ]0,1[ \\,:\\, T(Z)\\ge c_{\\alpha}\\}\\\\ \u0026=1-\\Psi\\left(\\frac{(\\bar{X}_n-\\bar{Y}_p)^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\right), \\end{align*} \\] où \\(\\Psi\\) désigne la fonction de répartition de la loi de Fisher de \\((1,n+p-2)\\) degrés de liberté.\n7. Construire un test de niveau \\(\\alpha\\) de l’hypothèse \\[ H_0 : \\mu_0\\le \\mu_1,\\quad\\text{contre}\\quad H_1 : \\mu_0\u003e\\mu_1. \\] On considère l’estimateur de la différence \\(\\mu_0-\\mu_1\\) donné par \\(T(Z):=\\frac{\\bar{X}_n-\\bar{Y}_p}{\\sqrt{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}}\\). On observe que sous \\(H_0\\), \\[ \\begin{align*} \\mathbb{P}_{\\theta}\\left(T(Z)\\ge \\sqrt{q_{1-\\alpha}^{F(1,n+p-2)}}\\right) \u0026\\le \\mathbb{P}_{\\theta}\\left(T(Z)-\\frac{\\mu_0-\\mu_1}{\\sqrt{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}}\\ge \\sqrt{q_{1-\\alpha}^{F(1,n+p-2)}}\\right)\\\\ \u0026\\le \\mathbb{P}_{\\theta}\\left(\\frac{[(\\bar{X}_n-\\bar{Y}_p)-(\\mu_0-\\mu_1)]^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\ge q_{1-\\alpha}^{F(1,n+p-2)}\\right)\\\\ \u0026= \\alpha. \\end{align*} \\] Ainsi, le test \\[ \\phi(Z):=\\mathbb{1}\\left\\{\\frac{\\bar{X}_n-\\bar{Y}_p}{\\sqrt{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}}\\ge \\sqrt{q_{1-\\alpha}^{F(1,n+p-2)}}\\right\\} \\] est un test de niveau \\(\\alpha\\) de l’hypothèse \\(H_0:\\mu_0\\le \\mu_1\\) contre \\(H_1:\\mu_0\u003e\\mu_1\\).\n8. Calculer la \\(p\\)-valeur du test. Dans notre cas, la statistique de test est \\(T(Z):=\\frac{\\bar{X}_n-\\bar{Y}_p}{\\sqrt{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}}\\) et la valeur critique du test est \\(c_{\\alpha}:=\\sqrt{q_{1-\\alpha}^{F(1,n+p-2)}}\\), qui est décroissante par rapport à \\(\\alpha\\in [0,1]\\). Ainsi, la \\(p\\)-valeur du test est \\[ \\begin{align*} \\hat{\\alpha}(Z) \u0026=\\inf\\{\\alpha\\in ]0,1[ \\,:\\, T(Z)\\ge c_{\\alpha}\\}\\\\ \u0026= \\begin{cases} 1-\\Psi\\left(\\frac{(\\bar{X}_n-\\bar{Y}_p)^2}{(\\frac{1}{n}+\\frac{1}{p})(nV_n+pW_p)}\\right), \u0026 \\text{si $\\bar{X}_n\u003e\\bar{Y}_p$;}\\\\ +\\infty, \u0026 \\text{sinon,} \\end{cases} \\end{align*} \\] où \\(\\Psi\\) désigne la fonction de répartition de la loi de Fisher de \\((1,n+p-2)\\) degrés de liberté.\n","description":"","tags":["Inférence Statistique"],"title":"Tests et Régions de Confiance","uri":"/posts/tests-et-regions-de-confiance/"},{"categories":["Algebra"],"content":"What’s next?\n— No.6 — See here.\n— No.5 — Shihe asked me the following problem on October 23, 2022. After a while, we exchanged our proofs, which I felt faithfully reflected the differences in the ways we do math.\nProblem [Existence of suitable orthogonal projections]\nLet \\(L\\) be an \\(n\\)-dimensional subspace of \\(\\mathbb{R}^{n+l}\\) (\\(n,l\\ge 1\\)) and \\(\\{v_1,\\cdots,v_{n+l}\\}\\) an arbitrary basis of \\(\\mathbb{R}^{n+l}\\). Show that there exist \\(1\\le j_1\u003c\\cdots\u003cj_n\\le n+l\\) such that the orthogonal projection \\(P_{j_1,\\cdots,j_n}\\) of \\(\\mathbb{R}^{n+l}\\) onto \\(\\text{span}\\{v_{j_1},\\cdots,v_{j_n}\\}\\) is injective on \\(L\\).\nProof I [Haosen]\nThe orthogonal complement of the kernel of \\(P_{j_1,\\cdots,j_n}|_L\\) is \\[\\begin{align*} \\ker(P_{j_1,\\cdots,j_n}|_L)^{\\perp} \u0026=\\left(\\ker(P_{j_1,\\cdots,j_n})\\cap L\\right)^{\\perp}\\\\ \u0026=\\left(\\text{span}(\\{v_{j_1},\\cdots,v_{j_n}\\})^{\\perp}\\cap L\\right)^{\\perp}\\\\ \u0026=\\text{span}(\\{v_{j_1},\\cdots,v_{j_n}\\})+L^{\\perp}. \\end{align*}\\] Therefore \\(P_{j_1,\\cdots,j_n}|_L\\) is injective iff \\(\\text{span}(\\{v_{j_1},\\cdots,v_{j_n}\\})+L^{\\perp}=\\mathbb{R}^{n+l}\\).\nLet \\(\\{w_1,\\cdots,w_l\\}\\) be a basis of the \\(l\\)-dimensional subspace \\(L^{\\perp}\\). There exists \\(A\\in M_{(n+l)\\times l}(\\mathbb{R})\\) of rank \\(l\\) such that \\[ (w_1\\ \\cdots\\ w_l)=(v_1\\ \\cdots\\ v_{n+l})A. \\] Now we note that there exists a sequence of elementary column operations that transforms \\(A\\) into a matrix with exactly \\(n\\) zero rows, in other words, there exists \\(Q\\in GL_{l}(\\mathbb{R})\\) such that \\(AQ\\) has exactly \\(n\\) zero rows, with the other \\(l\\) nonzero rows being linearly independent. We claim that the labels of the zero rows, denoted \\(1\\le j_1\u003c\\cdots\u003cj_n\\le n+l\\), satisfy the property in the statement of the problem. For clarity, denote by \\(1\\le i_1\u003c\\cdots\u003ci_l\\le n+l\\) the labels of the nonzero rows.\nIn fact, \\(L^{\\perp}=\\text{span}(\\{w_1,\\cdots,w_l\\})=\\) the column space of the matrix \\((w_1\\ \\cdots\\ w_l)=\\) the column space of the matrix \\((w_1\\ \\cdots\\ w_l)Q=\\) the column space of the matrix \\((v_1\\ \\cdots\\ v_{n+l})AQ=\\text{span}(\\{v_{i_1},\\cdots,v_{i_l}\\})\\). Therefore, \\(\\text{span}(\\{v_{j_1},\\cdots,v_{j_n}\\})+L^{\\perp}=\\text{span}(\\{v_1,\\cdots,v_{n+l}\\})=\\mathbb{R}^{n+l}\\), showing that \\(P_{j_1,\\cdots,j_n}|_L\\) is injective. \\(\\blacksquare\\)\nProof II [Shihe]\nShihe’s proof is more geometrically intuitive. For each \\(1\\le k\\le n+l\\), take a nonzero vector \\(w_k\\in \\mathbb{R}^{n+l}\\) such that \\(\\text{span}(\\{w_k\\})=\\text{span}(\\{v_1,\\cdots,v_{n+l}\\}\\setminus\\{v_k\\})^{\\perp}\\). We claim that \\(w_1,\\cdots,w_{n+l}\\) are linearly independent.\nIn fact, if \\(\\sum_{k=1}^{n+l}\\lambda_k w_k=0\\) (\\(\\lambda_1,\\cdots,\\lambda_{n+l}\\in \\mathbb{R}\\)), then by applying \\(\\langle \\cdot,v_k\\rangle\\) to both sides we get \\(\\lambda_k\\langle w_k,v_k\\rangle=0\\) (\\(k=1,\\cdots,n+l\\)). Clearly \\(\\langle w_k,v_k\\rangle\\neq 0\\), because otherwise \\(w_k\\) is orthogonal to every vector in the basis \\(\\{v_1,\\cdots,v_{n+l}\\}\\) and hence must be zero, contradiction. Thus \\(\\lambda_k=0\\ (k=1,\\cdots,n+l)\\), showing that \\(w_1,\\cdots,w_{n+l}\\) are linearly independent.\nSince \\(\\dim L=n\u003cn+l\\), there exists some \\(w_{k_1}\\) that is not in \\(L\\). The orthogonal projection of \\(\\mathbb{R}^{n+l}\\) along \\(\\text{span}(\\{w_{k_1}\\})\\), denoted by \\(P_{w_{k_1}}:\\mathbb{R}^{n+l}\\to \\mathbb{R}^{n+l}\\), induces an isomorphism when restricted to \\(L\\), as \\(\\ker(P_{w_{k_1}}|_L)=\\text{span}(\\{w_{k_1}\\})\\cap L=0\\). Moreover, \\[ \\text{im}(P_{w_{k_1}})=\\ker(P_{w_{k_1}})^{\\perp}=\\text{span}(\\{w_{k_1}\\})^{\\perp}=\\text{span}(\\{v_1,\\cdots,v_{n+l}\\}\\setminus\\{v_{k_1}\\}). \\]\nIf \\(l=1\\), we’re done. If \\(l\\ge 2\\), then \\(\\dim P_{w_{k_1}}(L)=n\u003cn+l-1\\) and so there exists \\(k_2\\neq k_1\\) such that \\(w_{k_2}\\notin P_{w_{k_1}}(L)\\). The orthogonal projection of \\(\\text{im}(P_{k_1})\\) along \\(\\text{span}(\\{w_{k_2}\\})\\), denoted by \\(P_{w_2}:\\text{im}(P_{w_1})\\to \\text{im}(P_{w_1})\\), induces an isomorphism when restricted to \\(P_{w_{k_1}}(L)\\), as \\(\\ker(P_{w_{k_2}}|_{P_{w_{k_1}}(L)})=\\text{span}(\\{w_{k_2}\\})\\cap P_{w_1}(L)=0\\). Moreover, \\[ \\text{im}(P_{w_{k_2}})=\\ker(P_{w_{k_2}})^{\\perp}=\\text{span}(\\{w_{k_2}\\})^{\\perp}=\\text{span}(\\{v_1,\\cdots,v_{n+l}\\}\\setminus\\{v_{k_1},v_{k_2}\\}). \\] Continue this process inductively until we get \\(P_{w_l}\\). Then \\[ P:\\mathbb{R}^{n+l}\\to \\mathbb{R}^{n+l}\\quad P(v):=P_{w_l}\\circ\\cdots\\circ P_{w_2}\\circ P_{w_1}(v) \\] is a well-defined orthogonal projection of \\(\\mathbb{R}^{n+l}\\) on \\(\\text{span}(\\{v_1,\\cdots,v_{n+l}\\}\\setminus\\{v_{k_1},\\cdots,v_{k_l}\\})\\). This completes the proof. \\(\\blacksquare\\)\n— No.4 — In early February 2023, I asked myself the following seemingly naive question.\nProblem [The Gramian determines the shape]\nLet \\(\\{v_1,\\cdots,v_s\\}\\) and \\(\\{w_1,\\cdots,w_s\\}\\) be two subsets of \\(\\mathbb{R}^n\\). Show that there exists \\(A\\in O(n)\\) such that \\(Av_i=w_i\\ (1\\le i\\le s)\\) iff \\(\\langle v_{i}, v_{j}\\rangle=\\langle w_{i}, w_{j}\\rangle\\ (1\\le i,j\\le s)\\), i.e., the two Gramians are equal.\n(Similarly, if \\(\\{v_1,\\cdots,v_s\\}\\) and \\(\\{w_1,\\cdots,w_s\\}\\) are two subsets of \\(\\mathbb{C}^n\\), then there exists \\(A\\in U(n)\\) such that \\(Av_i=w_i\\ (1\\le i\\le s)\\) iff \\(\\langle v_{i}, v_{j}\\rangle=\\langle w_{i}, w_{j}\\rangle\\ (1\\le i,j\\le s)\\), i.e., the two Gramians are equal.)\nProof [Haosen]\nWe summarize the idea of proving \\((\\Leftarrow)\\) as follows. Given the data \\(\\langle v_i,v_j \\rangle\\ (1\\le i,j\\le s)\\), we may focus on a maximal linearly independent subset of \\(\\{v_1,\\cdots,v_s\\}\\) to study the shape formed by these \\(s\\) vectors in \\(\\mathbb{R}^n\\). Suppose that \\(\\{v_{k_1},\\cdots,v_{k_r}\\}\\) is a maximal linearly independent subset of \\(\\{v_1,\\cdots,v_s\\}\\), then \\(\\{w_{k_1},\\cdots,w_{k_r}\\}\\) is automatically a maximal linearly independent subset of \\(\\{w_1,\\cdots,w_s\\}\\). Perform Gram-Schmidt on them simultaneously and extend the resulted orthonormal subsets to two orthonormal bases for \\(\\mathbb{R}^n\\). The associated change of coordinates matrix then fulfills the proof. (The Gram-Schmidt process can be embodied by QR decomposition.)\n\\((\\Rightarrow)\\): Obvious. \\[ \\begin{align*} G(w_1,\\cdots,w_s)\u0026=(w_{1}\\ \\cdots\\ w_{s})^{T}(w_{1}\\ \\cdots\\ w_{s})\\\\ \u0026=(v_{1}\\ \\cdots\\ v_{s})^{T}A^TA(v_{1}\\ \\cdots\\ v_{s})\\\\ \u0026=(v_{1}\\ \\cdots\\ v_{s})^{T}(v_{1}\\ \\cdots\\ v_{s})=G(v_1,\\cdots,v_s).\\end{align*} \\] \\((\\Leftarrow)\\): Note that \\[\\begin{align*}\\text{rank}(v_{1}\\ \\cdots\\ v_{s})\u0026=\\text{rank}\\, G(v_1,\\cdots,v_{s})\\\\ \u0026=\\text{rank}\\, G(w_1,\\cdots,w_s)=\\text{rank}(w_{1}\\ \\cdots\\ w_{s})\\end{align*}\\] Denote \\(r:=\\text{rank}(v_{1}\\ \\cdots\\ v_{s})\\). Without loss of generality, assume that \\(v_1,\\cdots,v_r\\) are linearly independent. We claim that \\(w_1,\\cdots,w_r\\) are linearly independent as well. Indeed, \\[ \\begin{align*} \\text{Null}\\,(w_1\\ \\cdots\\ w_r)\u0026=\\text{Null}\\,(w_1\\ \\cdots\\ w_r)^T(w_1\\ \\cdots\\ w_r)\\\\ \u0026=\\text{Null}\\,(v_1\\ \\cdots\\ v_r)^T(v_1\\ \\cdots\\ v_r)=\\text{Null}\\,(v_1\\ \\cdots\\ v_r)=0. \\end{align*} \\] Therefore, there exist \\(B,C\\in M_{r\\times (s-r)}(\\mathbb{R})\\) such that \\[ \\begin{align*} (v_1\\ \\cdots\\ v_s)\u0026=(v_1\\ \\cdots\\ v_r)(I_r\\ |\\ B),\\\\ (w_1\\ \\cdots\\ w_s)\u0026=(w_1\\ \\cdots\\ w_r)(I_r\\ |\\ C). \\end{align*} \\] Denote \\(G:=G(v_1,\\cdots,v_r)=G(w_1,\\cdots,w_r)\\). Since \\(\\text{rank}(G)=\\text{rank}(v_1\\ \\cdots\\ v_r)=r\\), the matrix \\(G\\) is invertible. Thus we have \\[ \\begin{pmatrix} I_r\\\\\\hline B^T \\end{pmatrix}\\,G\\,(I_r\\ |\\ B)=\\begin{pmatrix} I_r\\\\\\hline C^T \\end{pmatrix}\\,G\\,(I_r\\ |\\ C)\\implies GB=GC\\implies B=C. \\] Now, it suffices to find some \\(A\\in O(n)\\) such that \\[ A(v_1\\ \\cdots\\ v_r)=(w_1\\ \\cdots\\ w_r) \\] By QR decomposition, we have \\[ (v_1\\ \\cdots\\ v_r)=Q_1R_1,\\quad (w_1\\ \\cdots\\ w_r)=Q_2R_2 \\] where \\(R_i\\in M_{r\\times r}(\\mathbb{R})\\) is an upper triangular matrix with positive diagonal entries and \\(Q_i\\in M_{n\\times r}(\\mathbb{R})\\) satisfies \\(Q_i^TQ_i=I_r\\) (semi-orthogonal). Thus we have \\[ (Q_1R_1)^TQ_1R_1=(Q_2R_2)^TQ_2R_2\\implies R_1^TR_1=R_2^TR_2. \\] By the uniqueness of Cholesky decomposition for positive-definite matrices, we have \\(R_1=R_2\\). Indeed, \\(R_1R_2^{-1}=(R_1^{T})^{-1}R_2^T\\) is upper triangular and lower triangular, and hence a diagonal matrix, denoted by \\(D\\). Thus we have \\[ R_1=DR_2,\\ R_2^T=R_1^TD\\implies D=I_r\\implies R_1=R_2. \\]\nTherefore, it suffices to find some \\(A\\in O(n)\\) such that \\[ AQ_1=Q_2. \\] Since the columns of \\(Q_i\\) forms an orthonormal subset of \\(\\mathbb{R}^n\\) and thus extends to an orthonormal basis for \\(\\mathbb{R}^n\\), there exists \\(\\widetilde{Q}_i\\in O(n)\\) such that \\(\\widetilde{Q}_i=(Q_i\\ |\\ X_i)\\) for some \\(X_i\\in M_{n\\times (n-r)}(\\mathbb{R})\\). Define \\(A:=\\widetilde{Q}_2\\widetilde{Q}_1^{-1}\\). Then \\(A\\in O(n)\\) and \\(AQ_1=Q_2\\), as desired. \\(\\blacksquare\\)\n— No.3 — The next was one of the problems in my entrance examination of École Polytechnique, in November 2021. It estimates the size of a set of unit vectors that are “almost orthonormal”.\nProblem [Almost orthonormal]\nShow that if \\(v_1,\\cdots,v_m\\) are \\(m\\) unit vectors in \\(\\mathbb{C}^n\\) such that \\(|\\langle v_i,v_j \\rangle|\\le \\frac{1}{2\\sqrt{n}}\\) for any \\(i\\neq j\\), then \\(m\u003c2n\\).\nProof [Haosen, under the guidance of the examiner]\nLet \\(G:=G(v_1,\\cdots,v_m)\\) be the Gramian, i.e., \\(G(i,j)=\\langle v_i,v_j \\rangle\\) for \\(1\\le i,j\\le m\\). Define \\(H:=G-I_m\\). Since \\(H\\) is Hermitian, we have \\[ \\begin{align*} \\text{tr}(H^2)\u0026=\\text{tr}(H^*H)\\\\ \u0026=\\sum_{i,j=1}^{m}|H(i,j)|^2\\\\ \u0026=\\sum_{1\\le i\\neq j\\le m}|\\langle v_i,v_j\\rangle|^2\\\\ \u0026\\le (m^2-m)\\cdot (\\tfrac{1}{2\\sqrt{n}})^2=\\frac{m^2-m}{4n}. \\end{align*} \\] On the other hand, since \\(G\\) is positive semi-definite, \\(G\\) has \\(r:=\\text{rank}(G)=\\text{rank}(v_1\\ \\cdots\\ v_m)\\le n\\) positive eigenvalues, and the other \\(m-r\\) eigenvalues of \\(G\\) are all zero. (In fact, we only use the latter.) Consequently, the eigenvalues of \\(H\\) are \\(\\underbrace{\\lambda_1,\\cdots,\\lambda_r}_{\\in (-1,+\\infty)},\\underbrace{-1,\\cdots,-1}_{m-r}\\), and the eigenvalues of \\(H^2\\) are \\(\\underbrace{\\lambda_1^2,\\cdots,\\lambda_r^2}_{\\in [0,+\\infty)},\\underbrace{1,\\cdots,1}_{m-r}\\). Therefore, \\[ \\text{tr}(H^2)=\\sum_{i=1}^{r}\\lambda_i^2+(m-r)\\ge m-n. \\] Combining the two estimates, we conclude that \\[ m-n\\le \\frac{m^2-m}{4n}, \\] from which \\(m\u003c2n\\) follows. \\(\\blacksquare\\)\n— No.2 — From September 2020 to February 2021 and from October 2021 to December 2021, I volunteered as a tutor of linear algebra for ten or so freshman in Zhejiang University. Here is an interesting question raised by one of them.\nProblem [A singular square matrix with all its first minors nonvanishing]\nFor every \\(n\\ge 3\\), construct a singular square matrix \\(A\\) of order \\(n+1\\) such that all its first minors (i.e. \\(n\\times n\\) minors of \\(A\\)) are nonzero.\n— No.1 — In early May 2020, Liqin sent me a problem set of linear algebra (holiday homework from Peking University), and here is one of my favorites.\nProblem [Family of independent and mutually commutative upper triangular matrices]\nLet \\(h(n)\\) denote the maximum size of a family of independent and mutually commutative upper triangular matrices in \\(M_n(\\mathbb{R})\\). Prove that \\(h(n)\\le h(n-1)+n/2\\) for all \\(n\\ge 2\\).\n","description":"","tags":["Undergraduate","Linear Algebra","Problem Solving"],"title":"Problems in Linear Algebra","uri":"/posts/linear-algebra-problem-solving/"},{"categories":["Topology"],"content":" The Poincaré Lemma for de Rham Cohomology Coming soon…\nThe Poincaré Lemma for Compactly Supported Cohomology Here the goal is to explain the ideas behind the technical construction of the chain homotopy in the proof of the Poincaré lemma for compactly supported cohomology (cf. Bott-Tu, Differential Forms in Algebraic Topology).\nLet \\(M\\) be a smooth manifold. Then the product manifold \\(M\\times \\mathbb{R}^1\\) is a smooth manifold as well, and the projection \\(\\pi:M\\times \\mathbb{R}^1\\to M\\) is a smooth map. The pullback \\(\\pi^*:\\Omega^*(M)\\to \\Omega^*(M\\times \\mathbb{R}^1)\\) [1] does not map \\(\\Omega_c^*(M)\\) into \\(\\Omega_c^*(M\\times \\mathbb{R}^1)\\), so instead we consider the pushforward known as integration along the fiber \\[ \\begin{equation*} \\pi_*:\\Omega_c^*(M\\times \\mathbb{R}^1)\\to \\Omega_c^{*-1}(M)\\quad \\begin{cases} f(x,t)\\pi^*\\phi\\mapsto 0;\\\\ f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi\\mapsto \\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi, \\end{cases} \\end{equation*} \\] where \\(\\phi\\in \\Omega^*(M)\\) and \\(f\\in C_c^{\\infty}(M\\times \\mathbb{R};\\mathbb{R})\\). (These two types of elements in \\(\\Omega_c^*(M\\times \\mathbb{R}^1)\\) will soon be revisited.) It’s easy to check that \\(\\pi_*\\) is a chain map. We will show that the induced map \\(\\pi_*:H_c^*(M\\times \\mathbb{R}^1)\\to H_c^{*-1}(M)\\) is an isomorphism.\nTake \\(e=e(t)\\mathrm{d}{t}\\in \\Omega_c^1(\\mathbb{R}^1)\\) such that \\(\\displaystyle\\int_{\\mathbb{R}^1} e=1\\). Define \\[ \\begin{equation*} e_*:\\Omega_c^*(M)\\to \\Omega_c^{*+1}(M\\times \\mathbb{R}^1)\\quad \\psi\\mapsto e(t)\\mathrm{d}{t}\\wedge\\pi^*\\psi, \\end{equation*} \\] where the map \\(t\\mapsto e(t)\\) has been identified with the composition \\((x,t)\\mapsto t\\mapsto e(t)\\). One quickly checks that \\(e_*\\) is a chain map, and \\(\\pi_*\\circ e_*=\\text{id}\\) on \\(\\Omega_c^*(M)\\). We will show that there is a chain homotopy \\(K\\) connecting \\(e_*\\circ \\pi_*\\) and \\(\\text{id}\\) on \\(\\Omega_c^*(M\\times \\mathbb{R}^1)\\), so that the induced map \\(e_*:H_c^{*}(M)\\to H_c^{*+1}(M\\times \\mathbb{R}^1)\\) is the inverse of \\(\\pi_*:H_c^*(M\\times \\mathbb{R}^1)\\to H_c^{*-1}(M)\\).\nWe shall construct the desired chain homotopy \\(K:\\Omega_c^*(M\\times \\mathbb{R}^1)\\to \\Omega_c^{*-1}(M\\times \\mathbb{R}^1)\\) from the basic relation \\[ \\begin{equation*} \\text{id}-e_*\\circ \\pi_*=\\mathrm{d}K+K \\mathrm{d}. \\end{equation*} \\] To proceed, interpret this relation on the aforementioned two types of elements in \\(\\Omega_c^*(M\\times \\mathbb{R}^1)\\), and be reminded that computations will be done w.r.t. some specific local coordinates.\nFor \\(f(x,t)\\pi^*\\phi\\), we require \\[ \\begin{equation*} f(x,t)\\pi^*\\phi=\\mathrm{d}K(f(x,t)\\pi^*\\phi)+K \\left[\\left(\\displaystyle\\sum_i \\dfrac{\\partial f}{\\partial x^i}(x,t)\\mathrm{d}{x^i}+\\dfrac{\\partial f}{\\partial t}(x,t)\\mathrm{d}{t}\\right)\\wedge \\pi^*\\phi+f(x,t)\\pi^*\\mathrm{d}{\\phi}\\right]. \\end{equation*} \\] For \\(f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi\\), we require \\[ \\begin{align*} \u0026f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi-e(t)\\mathrm{d}{t}\\wedge\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]\\\\ \u0026=\\mathrm{d}K(f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi)+K \\left[-\\displaystyle\\sum_i \\dfrac{\\partial f}{\\partial x^i}(x,t)\\mathrm{d}{t}\\wedge\\mathrm{d}{x^i}\\wedge\\pi^*\\phi-f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\mathrm{d}{\\phi}\\right]. \\end{align*} \\] We observe that when \\(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t=0\\), or equivalently, \\(f\\in \\dfrac{\\partial }{\\partial t}(C_c^{\\infty}(M\\times \\mathbb{R}^1))\\), the two requirements become very close. An immediate guess follows: \\[ \\begin{equation*} K_1(f(x,t)\\pi^*\\phi)=0,\\quad K_1(f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi)=\\left(\\displaystyle\\int_{-\\infty}^{t} f(x,t) \\,\\mathrm{d}t\\right)\\pi^*\\phi, \\end{equation*} \\] whenever \\(\\phi\\in \\Omega^*(M)\\) and \\(f\\in C_c^*(M\\times \\mathbb{R}^1)\\). While \\(K_1\\) solves the first requirement, it does not reconcile with the second: \\[ \\begin{align*} \u0026\\mathrm{d}K_1(f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi)+K_1 \\left[-\\displaystyle\\sum_i \\dfrac{\\partial f}{\\partial x^i}(x,t)\\mathrm{d}{t}\\wedge\\mathrm{d}{x^i}\\wedge\\pi^*\\phi-f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\mathrm{d}{\\phi}\\right]\\\\ =\u0026\\mathrm{d}\\left[\\left(\\displaystyle\\int_{-\\infty}^{t} f(x,t) \\,\\mathrm{d}t\\right)\\pi^*\\phi\\right]-\\displaystyle\\sum_i \\left(\\displaystyle\\int_{-\\infty}^{t} \\dfrac{\\partial f}{\\partial x^i}(x,t) \\,\\mathrm{d}t\\right)\\mathrm{d}{x^i}\\wedge \\pi^*\\phi-\\left(\\displaystyle\\int_{-\\infty}^{t} f(x,t) \\,\\mathrm{d}t\\right)\\pi^*\\mathrm{d}{\\phi}\\\\ =\u0026f(x,t)\\mathrm{d}{t}\\wedge \\pi^*\\phi. \\end{align*} \\] However, the computation also indicates that we are not far away from success, with only one term involving \\(e=e(t)\\mathrm{d}{t}\\) missing.\nAs a remedy, consider \\(K_2=K-K_1\\). It suffices to construct \\(K_2\\) in the same fashion:\nFor \\(f(x,t)\\pi^*\\phi\\), we require \\[ \\begin{equation*} 0=\\mathrm{d}K_2(f(x,t)\\pi^*\\phi)+K_2 \\left[\\left(\\displaystyle\\sum_i \\dfrac{\\partial f}{\\partial x^i}(x,t)\\mathrm{d}{x^i}+\\dfrac{\\partial f}{\\partial t}(x,t)\\mathrm{d}{t}\\right)\\wedge \\pi^*\\phi+f(x,t)\\pi^*\\mathrm{d}{\\phi}\\right]. \\end{equation*} \\] For \\(f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi\\), we require \\[ \\begin{align*} \u0026-e(t)\\mathrm{d}{t}\\wedge\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]\\\\ \u0026=\\mathrm{d}K_2(f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi)+K_2 \\left[-\\displaystyle\\sum_i \\dfrac{\\partial f}{\\partial x^i}(x,t)\\mathrm{d}{t}\\wedge\\mathrm{d}{x^i}\\wedge\\pi^*\\phi-f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\mathrm{d}{\\phi}\\right]. \\end{align*} \\] It is again natural to set \\(K_2(f(x,t)\\pi^*\\phi)=0\\), and so the first requirement simplifies to \\[ \\begin{equation*} K_2 \\left(\\dfrac{\\partial f}{\\partial t}(x,t)\\mathrm{d}{t}\\wedge \\pi^*\\phi\\right)=0. \\end{equation*} \\] Now we focus on the second requirement. Note that \\[ \\begin{align*} \u0026\\hphantom{=}e(t)\\mathrm{d}{t}\\wedge\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]\\\\ \u0026=\\mathrm{d}\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\wedge\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]+\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\mathrm{d}\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]\\\\ \u0026\\hphantom{=\\mathrm{d}\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\wedge\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]}-\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\mathrm{d}\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]\\\\ \u0026=\\mathrm{d}\\left\\{\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right]\\right\\}\\\\ \u0026\\hphantom{=}-\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\pi^*\\left[\\displaystyle\\sum_i \\left(\\displaystyle\\int_{-\\infty}^{\\infty} \\dfrac{\\partial f}{\\partial x^i}(x,t) \\,\\mathrm{d}t\\right)\\mathrm{d}{x^i}\\wedge \\phi+\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\mathrm{d}{\\phi}\\right]. \\end{align*} \\] An immediate guess follows: \\[ \\begin{equation*} K_2(f(x,t)\\mathrm{d}{t}\\wedge \\pi^*\\phi)=-\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right], \\end{equation*} \\] which indeed solves the second requirement, and together with \\(K_2(f(x,t)\\pi^*\\phi)=0\\), solves the first, since \\[ \\begin{equation*} \\displaystyle\\int_{-\\infty}^{\\infty} \\dfrac{\\partial f}{\\partial t}(x,t) \\,\\mathrm{d}t=0. \\end{equation*} \\]\nConclusion: The map \\(K:\\Omega_c^{*}(M\\times \\mathbb{R}^1)\\to \\Omega_c^{*-1}(M\\times \\mathbb{R}^1)\\) defined by \\[ \\begin{equation*} \\begin{cases} f(x,t)\\pi^*\\phi\\mapsto 0\\\\ f(x,t)\\mathrm{d}{t}\\wedge\\pi^*\\phi\\mapsto \\left(\\displaystyle\\int_{-\\infty}^{t} f(x,t) \\,\\mathrm{d}t\\right)\\pi^*\\phi-\\left(\\displaystyle\\int_{-\\infty}^{t} e(t) \\,\\mathrm{d}t\\right)\\pi^*\\left[\\left(\\displaystyle\\int_{-\\infty}^{\\infty} f(x,t) \\,\\mathrm{d}t\\right)\\phi\\right] \\end{cases} \\end{equation*} \\] is a chain homotopy connecting \\(e_*\\circ \\pi_*\\) and \\(\\text{id}\\) on \\(\\Omega_c^*(M\\times \\mathbb{R}^1)\\), and consequently the maps \\[ \\begin{equation*} H_c^{*+1}(M\\times \\mathbb{R})\\begin{matrix}\\xrightarrow{\\pi_*}\\\\ \\xleftarrow[e_*]{}\\end{matrix}H_c^*(M) \\end{equation*} \\] are isomorphisms.\nAs a corollary, for each positive integer \\(n\\), there holds \\[ \\begin{equation*} H_c^*(\\mathbb{R}^n)=\\begin{cases} \\mathbb{R}, \u0026\\text{in dimension $n$;}\\\\ 0, \u0026\\text{elsewhere,} \\end{cases} \\end{equation*} \\] where the isomorphism \\(H_c^*(\\mathbb{R}^n)\\xrightarrow{\\cong}\\mathbb{R}\\) is given by iterated \\(\\pi_*\\), i.e., by integration over \\(\\mathbb{R}^n\\), thanks to Fubini’s theorem. Besides, by iterating \\(e_*\\), we see that a generator of \\(H_c^n(\\mathbb{R}^n)\\) is represented by a bump \\(n\\)-form \\(\\alpha=\\alpha(x)\\mathrm{d}{x^1}\\wedge\\cdots\\wedge \\mathrm{d}{x^n}\\) with \\(\\displaystyle\\int_{\\mathbb{R}^n} \\alpha=1\\), whose support can be made as small as possible.\nThe \\(\\overline{\\partial}\\)-Poincaré Lemma (aka Dolbeault-Grothendieck Lemma) Coming soon…\nTo be explicit, if \\(\\phi\\in \\Omega^r(M)\\) is represented as \\(\\sum_{i_1\u003c\\cdots\u003ci_r}\\phi_{i_1\\cdots i_r}(x)\\mathrm{d}{x^{i_1}}\\wedge\\cdots\\wedge \\mathrm{d}{x^{i_r}}\\) w.r.t. some chosen local coordinates on \\(M\\), then \\(\\pi^*\\phi\\in \\Omega^r(M\\times \\mathbb{R})\\) is simply represented as \\(\\sum_{i_1\u003c\\cdots\u003ci_r}\\phi_{i_1\\cdots i_r}\\circ \\pi(x,t)\\mathrm{d}{x^{i_1}}\\wedge\\cdots\\wedge \\mathrm{d}{x^{i_r}}\\) w.r.t. the associated local coordinates on \\(M\\times \\mathbb{R}^1\\).↩︎\n","description":"","tags":["Algebraic Topology","Differentiable Manifolds","Complex Geometry"],"title":"The Poincaré Lemmas","uri":"/posts/poincare-lemmas/"},{"categories":["Geometry"],"content":"Where the whole story begins.\nPart I : Tangent Spaces \u0026 Cotangent Spaces Let \\(M\\) be a smooth \\(m\\)-manifold and \\(p\\in M\\). We will define the tangent space and the cotangent space of \\(M\\) at \\(p\\).\nTangent Space \\(T_pM\\) — The Geometric Approach Choose a local chart \\((U,\\varphi)\\) centered at \\(p\\).\nSet \\(C_p:=\\{\\alpha\\in C^{\\infty}(I;M) \\,|\\, \\text{$I$ is an open interval containing $0$}, \\alpha(0)=p\\}\\). Define \\(T_pM:=C_p/\\sim\\), where \\(\\sim\\) is an equivalence relation on \\(C_p\\) defined by \\(\\alpha\\sim \\beta:\\iff (\\varphi\\circ \\alpha)'(0)=(\\varphi\\circ \\beta)'(0)\\). If \\((V,\\psi)\\) is another local chart centered at \\(p\\), then we have \\((\\psi\\circ \\alpha)'(0)=(\\psi\\circ \\varphi^{-1})'(0) (\\varphi\\circ \\alpha)'(0)\\). Therefore the definition of the equivalence relation \\(\\sim\\) does not depend on the choice of the local chart, neither does the definition of \\(T_pM\\). The 1-1 correspondence \\(T_pM\\ni [\\alpha]\\mapsto (\\varphi\\circ \\alpha)'(0)\\in \\mathbb{R}^m\\) induces an \\(m\\)-dimensional real vector space structure on \\(T_pM\\). The elements of \\(T_pM\\) (i.e. the equivalence classes of curves passing through \\(p\\)) are called tangent vectors, and \\(T_pM\\) is called the tangent space of \\(M\\) at \\(p\\). Denote by \\(\\{\\left.\\tfrac{\\partial }{\\partial x^1}\\right|_p, \\cdots, \\left.\\tfrac{\\partial }{\\partial x^m}\\right|_p\\}\\) the base of \\(T_pM\\) corresponding to \\(\\{e_1,\\cdots,e_m\\}\\) in the chosen local chart.\nRemark [Tangent vectors as directional derivatives] The following observation motivates the definition of cotangent vectors: we can view tangent vectors as operators taking directional derivatives of smooth functions. For every \\(\\gamma\\in C_p\\), whenever \\(f\\) is a real-valued \\(C^{\\infty}\\) function on an open neighborhood of \\(p\\), define \\[\\begin{equation*} [\\gamma]f:=D_{\\gamma}(f)=\\left.\\dfrac{\\mathrm{d}}{\\mathrm{d}t}\\right|_{t=0}(f\\circ \\gamma)(t). \\end{equation*}\\] To justify this definition we note that \\[\\begin{equation*} D_{\\gamma}(f)=\\nabla(f\\circ \\varphi^{-1})(0)\\cdot (\\varphi\\circ \\gamma)'(0), \\end{equation*}\\] which shows that \\(D_\\gamma\\) (taking derivatives along \\(\\gamma\\)) is completely determined by the equivalence class of \\(\\gamma\\), i.e., the tangent vector \\([\\gamma]\\). This identity also shows that \\[\\begin{equation*} \\left.\\dfrac{\\partial }{\\partial x^i}\\right|_p f=\\partial_i (f\\circ \\varphi^{-1})(0) \\end{equation*}\\] and moreover: \\(([\\alpha]+[\\beta])f=[\\alpha]f+[\\beta]f, (\\lambda[\\alpha])f=\\lambda([\\alpha]f)\\). Soon we will see the precise duality behind these ideas.\nWe also mention that \\([\\gamma](f+g)=[\\gamma]f+[\\gamma]g, [\\gamma](\\lambda f)=\\lambda([\\gamma]f)\\), and \\([\\gamma](fg)=([\\gamma]f)g(p)+f(p)([\\gamma]g)\\). This leads to the treatment of tangent spaces as so called derivations, which we will skip for now.\nOf course we can just define the cotangent space \\(T_p^*M\\) to be the dual of the tangent space \\(T_pM\\). But it will be beneficial to explore the hidden duality seen above. In the following, we first define cotangent spaces using a standard algebraic approach and then specify the natural duality between \\(T_pM\\) and \\(T_p^*M\\).\nCotangent Space \\(T_p^*M\\) — The Algebraic Approach Set \\(F_p:=\\{(f,U) \\,|\\, \\text{$U$ is an open neighborhood of $p$}, f\\in C^{\\infty}(U;\\mathbb{R})\\}/\\sim\\), where the equivalence relation is as follows: \\((f,U)\\sim (g,V)\\) iff there exists an open neighborhood \\(W\\subset U\\cap V\\) such that \\(f|_W=g|_W\\). The elements of \\(F_p\\) are called germs (of the sheaf of smooth functions on \\(M\\)). In the space of germs at \\(p\\), define \\([f,U]+[g,V]:=[f+g,U\\cap V]\\) and \\([f,U]\\cdot [g,V]:=[fg,U\\cap V]\\). Then \\((F_p,+,\\cdot)\\) is a local ring whose unique maximal ideal is \\(\\mathfrak{m}_p:=\\{[f]\\in F_p \\,|\\, f(p) = 0\\}\\) and whose residue field is \\(\\mathbb{R}\\). Define the cotangent space of \\(M\\) at \\(p\\) to be \\(T_p^*M:=\\mathfrak{m}_p/\\mathfrak{m}_p^2\\).\nLet’s show that \\(T_p^*M\\) is naturally a real vector space of dimension \\(m\\). Let \\(x^1,\\cdots,x^m\\) be a local coordinate system centered at \\(p\\). One can easily show that \\[\\begin{equation*} \\mathfrak{m}_p=\\{[f]\\in F_p \\,|\\, \\exists [f_i]\\in F_p \\text{ s.t. } f= \\textstyle\\sum x^if_i\\}=\\langle [x^1],\\cdots,[x^m] \\rangle, \\end{equation*}\\] which is known as Hadamard’s lemma. Consider the map \\[\\begin{equation*} \\theta:\\mathfrak{m}_p\\to \\mathbb{R}^n\\quad [\\textstyle\\sum x^if_i]\\mapsto (f_1(p),\\cdots,f_m(p)). \\end{equation*}\\] To verify that \\(\\theta\\) is well-defined, suppose that \\([\\sum x^if_i]=[\\sum x^ig_i]\\). In a suitable open neighborhood of \\(p\\), we have \\(f_i=g_i\\) on the subset \\((\\cap_{j\\neq i} \\{x_j=0\\})\\cap \\{x_i\\neq 0\\}\\). By continuity, we get \\(f_i(p)=g_i(p)\\), although \\([f_i]\\) may not equal to \\([g_i]\\). Therefore \\(\\theta\\) is a well-defined map, and moreover is a surjective homomorphism of \\(\\mathbb{R}\\)-modules. Now we conclude from \\(\\ker(\\theta)=\\mathfrak{m}_p^2\\) that \\(T_p^*M=\\mathfrak{m}_p/\\mathfrak{m}_p^2\\cong \\mathbb{R}^m\\) (as \\(\\mathbb{R}\\)-modules) and that \\(T_p^*M=\\text{span}\\{\\mathrm{d}x^1,\\cdots,\\mathrm{d}x^m\\}\\), where \\(\\mathrm{d}x^i:=[[x^i]]=[x^i]+\\mathfrak{m}_p^2\\). The elements of \\(T_p^*M\\) are called cotangent vectors.\nRemark [Duality] Suppose that the local coordinate system is associated with the local chart used before, i.e., \\(\\varphi=(x^1,\\cdots,x^m)\\), or equivalently \\(x^i\\circ \\varphi^{-1}(x_1,\\cdots,x_m)=x_i\\). For any \\([f]\\in \\mathfrak{m}_p\\), write \\(f=\\sum x^if_i\\), then by linearity and Leibniz’s rule we have \\(\\left.\\tfrac{\\partial }{\\partial x^j}\\right|_p f =\\sum_i [(\\left.\\tfrac{\\partial }{\\partial x^j}\\right|_px^i)f_i(p)+x^i(p)(\\left.\\tfrac{\\partial }{\\partial x^j}\\right|_p f_i)] =\\sum_i \\partial_j(x^i\\circ \\varphi^{-1})(0)f_i(p) =f_j(p)\\). More generally, \\[\\begin{equation*} [\\gamma]f=(\\varphi\\circ \\gamma)'(0)\\cdot\\theta([f]),\\quad \\forall [\\gamma]\\in T_pM. \\end{equation*}\\] Therefore we have a canonical pairing between \\(T_pM\\) and \\(T_p^*M\\): \\[\\begin{equation*} [\\gamma][[f]]:=[\\gamma]f,\\quad\\forall ([\\gamma],[[f]])\\in T_pM\\times T_p^*M. \\end{equation*}\\] Since \\(\\left.\\tfrac{\\partial }{\\partial x^i}\\right|_p \\mathrm{d}x^j = \\left.\\tfrac{\\partial }{\\partial x^i}\\right|_p x^j=\\partial_i(x^j\\circ \\varphi^{-1})(0)=\\delta_i^j\\), the canonical pairing is perfect, showing that the tangent space \\(T_pM\\) and the cotangent space \\(T_p^*M\\) are canonically dual to each other.\nPart II : Tangent Maps \u0026 Cotangent Maps Coming soon…\nPart III : Tangent Bundles \u0026 Cotangent Bundles Coming soon…\nPart IV : Vector Fields \u0026 Lie Brackets Coming soon…\nExercises Let \\(M^m\\) and \\(N^n\\) be two smooth manifolds. Then \\(M\\times N\\) is naturally a smooth \\((m+n)\\)-manifold. We have two canonical isomorphisms: \\(T_{(p,q)}(M\\times N)\\cong T_pM\\times T_qN\\) and \\(T_{(p,q)}^*(M\\times N)\\cong T_p^*M\\times T_q^*N\\).\nLet \\(M\\) be a smooth \\(m\\)-manifold. Then the tangent space of the diagonal of \\(M\\times M\\) at a point \\((p,p)\\) is the diagonal of \\(T_pM\\times T_pM\\).\nThe special linear group \\(\\text{SL}(n,\\mathbb{R})\\) is a Lie group of dimension \\(n^2-1\\). The Lie algebra \\(\\mathfrak{sl}(n,\\mathbb{R}):=T_{I_n}(\\text{SL}(n,\\mathbb{R}))\\) consists of all \\(n\\times n\\) matrices over \\(R\\) with vanishing trace. The Lie bracket is given by the commutator. The exponential map from \\(\\mathfrak{sl}(n,\\mathbb{R})\\) to \\(\\text{SL}(n,\\mathbb{R})\\) is well-defined since we have \\(\\det(e^A)=e^{\\text{tr}(A)}\\) for any \\(A\\in M_{n\\times n}(\\mathbb{R})\\).\nPart V : Riemannian Metrics Coming soon…\nPart VI : Levi-Civita Connections Coming soon…\nPart VII : Exponential Maps Coming soon…\nExercises ","description":"","tags":["Differentiable Manifolds"],"title":"Tangent Story","uri":"/posts/tangent/"},{"categories":["Geometry"],"content":"Coming soon…\n","description":"","tags":["Differential Geometry","Riemannian Geometry"],"title":"Riemann Curvature Tensor","uri":"/posts/curvature/"}]
